{
  "issues": [
    {
      "title": "Prevent remote build tokens from being sent over insecure gRPC connections",
      "kind": "Security concerns",
      "files": [
        "src/remote/utils.go",
        "src/remote/remote.go"
      ],
      "short_description": "The remote execution client can attach a pre-shared token as per-RPC credentials even when the connection is insecure, and the credential provider explicitly advertises that it does *not* require transport security. This can leak tokens if a user misconfigures Remote.Secure or points at a non\u2011TLS endpoint.\n\n",
      "recommended_action": "1. Update the token credential provider:\n   - In `src/remote/utils.go` (the `tokenCredProvider` implementation), change `RequireTransportSecurity()` to return `true` so that gRPC will refuse to attach these credentials on non\u2011TLS connections.\n2. Guard per-RPC cred attachment on TLS being enabled:\n   - In the dial/`dialOpts` logic (also in `src/remote/utils.go`), only append `grpc.WithPerRPCCredentials(...)` when TLS creds were configured (or when `Remote.Secure` is true and TLS options have been set up in `src/remote/remote.go`).\n3. Make plaintext-token use an explicit, documented opt-in (if you truly need it for certain service-mesh deployments):\n   - Add a config flag such as `remote.allow_plaintext_token` (default: false).\n   - When this flag is true, allow `RequireTransportSecurity()` to return false or add a separate credential that is clearly marked as insecure, and warn loudly in logs.\n4. Add tests:\n   - Unit test that token creds are not attached when using `insecure.NewCredentials()`.\n   - Unit test that an insecure configuration with tokens either fails fast (preferred) or logs a clear, explicit warning.\n\nThis closes a direct security footgun while keeping flexibility for advanced deployments."
    },
    {
      "title": "Add sane, configurable timeouts for remote Execute / WaitExecution RPCs",
      "kind": "Security concerns",
      "files": [
        "src/remote/remote.go"
      ],
      "short_description": "The remote execution client configures 0 (infinite) timeouts for critical RPCs like `Execute` and `WaitExecution` in the remote API SDK client setup. This can cause builds or CI to hang indefinitely if a worker or network path stalls.\n\n",
      "recommended_action": "1. Identify the timeout configuration:\n   - In `src/remote/remote.go`, locate where `client.NewClient` is called with `client.RPCTimeouts(...)` and timeouts for `\"Execute\"` and `\"WaitExecution\"` are set to `0`.\n2. Introduce configuration options:\n   - Add fields to the `Remote` section of the config (e.g. `execute_timeout`, `wait_execution_timeout`) with sensible defaults (e.g. `30m` for `Execute`, or some proportion of target/test timeout; maybe `10m` for `WaitExecution`).\n3. Wire config into client setup:\n   - Replace hard-coded `0` values in `client.RPCTimeouts` with values pulled from config, interpreting `0` as \"infinite\" only when explicitly configured.\n4. Add logging and failure behaviour:\n   - When a timeout is hit, surface a clear error including which RPC timed out and the configured limit.\n   - Optionally add a separate metric (e.g. `remote_execute_timeout_count`) to help operators tune timeouts.\n5. Tests:\n   - Unit test that default configuration yields non-zero timeouts.\n   - Unit test that a custom config correctly overrides the timeouts.\n\nThis improves robustness and prevents stuck builds without changing semantics for users who explicitly opt into infinite timeouts."
    },
    {
      "title": "Make cache store/retrieve operations atomic and error-reporting across backends",
      "kind": "Security concerns",
      "files": [
        "src/cache/cache.go",
        "src/cache/http_cache.go",
        "src/cache/dir_cache.go",
        "src/cache/cmd_cache.go"
      ],
      "short_description": "Cache backends (dir, HTTP, cmd) currently duplicate tar/unpack logic, write directly into final locations, and the multiplexer hides store errors. This can lead to partially written outputs, undetected cache corruption, and confusing behaviour when one backend fails.\n\n",
      "recommended_action": "1. Centralize tar/untar logic:\n   - Introduce a small internal helper (e.g. `cache/tarutil.go`) with functions like:\n     - `func CreateTar(w io.Writer, files []string, root string) error`\n     - `func UnpackTar(r io.Reader, dest string) error`\n   - Refactor:\n     - `src/cache/dir_cache.go` (`storeCompressed`, `retrieveCompressed`)\n     - `src/cache/http_cache.go` (`readTar`, upload helpers)\n     - `src/cache/cmd_cache.go` (tar handling around Store/Retrieve)\n     to use these helpers instead of each reimplementing tar logic.\n\n2. Make retrieval atomic:\n   - For each backend, change retrieval to unpack into a temporary directory or temp file under the cache root, then move/rename into the final location only after successful extraction.\n   - On error during unpack, ensure temporary data is cleaned up and never partially overwrites final outputs.\n\n3. Stop swallowing store errors in the cache multiplexer:\n   - In `src/cache/cache.go`, update `Store`/`storeUntil` so that backend `Store` functions return an `error` instead of being fire-and-forget goroutines whose failures are ignored.\n   - Decide on policy: e.g. if any configured primary backend fails, return an error to the caller; for secondary backends, at least log metrics and warnings.\n\n4. Honour the HTTP cache cancellation TODO:\n   - In `src/cache/http_cache.go` near the existing TODO about cancelling the request, wire the write-side errors to cancel the HTTP request context or close the body so the server isn\u2019t left hanging.\n\n5. Tests:\n   - Add tests that simulate mid-stream errors during tar/unpack and verify that:\n     - Final output is either fully present and correct or not present at all.\n     - Store failures from backends are surfaced.\n\nThese changes reduce the chance of subtle cache corruption and make cache failures observable and debuggable."
    },
    {
      "title": "Harden sandbox directory configuration and mounting (SANDBOX_DIRS)",
      "kind": "Security concerns",
      "files": [
        "src/sandbox/sandbox_linux.go",
        "src/core/build_env.go"
      ],
      "short_description": "On Linux, configured sandbox directories from the `SANDBOX_DIRS` environment variable (backed by config) are mounted as tmpfs with minimal validation. Current checks only special-case some paths (like `/tmp`), leaving room for misconfiguration or abuse to mount over sensitive locations or escape intended sandboxes.\n\n",
      "recommended_action": "1. Tighten validation of sandbox directories:\n   - In `src/sandbox/sandbox_linux.go` (`sandboxDir` and `mountSandboxDirs`):\n     - Require paths to be absolute (`filepath.IsAbs`), then normalize with `filepath.Clean`.\n     - Reject obviously dangerous targets (e.g. `/`, `/etc`, `/var`, `/usr`, or anything outside an allowed base prefix such as the repo root or `plz-out`).\n     - Consider rejecting paths containing `..` or symlinks that resolve outside the allowed base.\n2. Validate configuration at load-time:\n   - In `src/core/build_env.go` where `SANDBOX_DIRS` is constructed from `state.Config.Sandbox.Dir`, perform validation when reading config so invalid values fail fast with a clear error message.\n3. Document behaviour:\n   - Update config/docs to clarify allowable sandbox directory values and the fact that unsafe values will be rejected.\n4. Tests:\n   - Unit tests for `sandboxDir` covering:\n     - Allowed paths under `plz-out` / repo root.\n     - Rejected dangerous paths (root, `/etc`, relative paths, or those containing `..`).\n\nThis closes off a class of misconfiguration issues that can undermine the isolation guarantees that Please\u2019s sandbox is expected to provide."
    },
    {
      "title": "Eliminate panic/recover-based control flow in core command replacements",
      "kind": "Non-standard or antipatterns",
      "files": [
        "src/core/command_replacements.go"
      ],
      "short_description": "`src/core/command_replacements.go` uses panics extensively to signal expected error conditions and wraps the main entrypoint in a `defer`/`recover` block that converts panics to errors. There\u2019s even a TODO to remove panics. This pattern makes control flow harder to reason about, can mask genuine programmer errors, and complicates testing.\n\n",
      "recommended_action": "1. Replace panics with explicit errors in replacement helpers:\n   - Identify helpers like `replaceSequence`, `replaceSequenceLabel`, and others that currently `panic` on:\n     - Invalid build labels (e.g. `TryParseBuildLabel` failures).\n     - Referring to targets that are not dependencies.\n     - Unsupported constructs (e.g. `$(worker)` usage) or malformed replacement syntax.\n   - Change them to return `(string, error)` (or a well-named result type) instead of panicking.\n\n2. Remove the global `defer recover` wrapper:\n   - The top-level functions (e.g. `ReplaceSequences`, `ReplaceTestSequences`) already return `(string, error)`; use those error returns instead of catching panics.\n   - After refactoring helpers, delete the `defer func() { if r := recover(); r != nil { ... } }()` blocks.\n\n3. Make error reporting explicit at call sites:\n   - Update all callers of `ReplaceSequences` / `ReplaceTestSequences` to handle returned errors appropriately (propagate upwards, or convert into user-facing error messages at the CLI layer; avoid `log.Fatalf` inside library code).\n\n4. Add targeted tests:\n   - Tests for malformed replacement syntax, invalid labels, and non-dependency references should assert that a structured error is returned (not a panic), with an informative message including the original command.\n\nThis refactor replaces a non-idiomatic panic/recover pattern with straightforward error handling, improving maintainability and safety in a core code path used to build actual commands executed by Please."
    },
    {
      "title": "Reduce panic-as-control-flow in BUILD language parser and interpreter",
      "kind": "Non-standard or antipatterns",
      "files": [
        "src/parse/asp/errors.go",
        "src/parse/asp/grammar_parse.go",
        "src/parse/asp/interpreter.go",
        "src/parse/asp/objects.go",
        "src/parse/asp/builtins.go"
      ],
      "short_description": "The BUILD language parser/interpreter (under `src/parse/asp`) relies heavily on panics to signal parse and evaluation errors, with a few central recovery points converting panics back into errors. This is similar in spirit to the command replacement issue but spread across a much larger and more complex subsystem.\n\n",
      "recommended_action": "1. Introduce explicit error types for parse/eval errors:\n   - Define types like `type ParseError struct { Pos Position; Msg string }` and `type EvalError struct { Pos Position; Msg string }`, along with helpers to attach stack-frame information.\n\n2. Refactor core helpers to return errors, not panic:\n   - Files to focus on:\n     - `src/parse/asp/errors.go` (`fail` currently panics).\n     - `src/parse/asp/grammar_parse.go` (top-level parse functions with `recover` guards).\n     - `src/parse/asp/interpreter.go` (interpreter methods that use `defer recover` and scope methods like `(*scope).Error` that panic).\n     - `src/parse/asp/objects.go` and `src/parse/asp/builtins.go` where object operations `panic` on type or index errors.\n   - Change these to return `(value, error)` or just `error` and propagate up.\n\n3. Minimize reliance on global `recover` wrappers:\n   - Keep a very small number of `recover` calls only around genuinely unexpected failures (programming bugs), not routine user-facing errors in BUILD files.\n\n4. Testing and migration:\n   - Start with a narrow surface (e.g. simple expression parsing or a subset of builtins), convert those to error returns, and add tests.\n   - Expand gradually, ensuring that existing user-facing error messages remain as friendly as possible.\n\nThis is a bigger, staged refactor but will pay off in clearer, more reliable error handling in one of the most central parts of Please: interpreting BUILD files."
    },
    {
      "title": "Replace goroutine+recover send patterns on channels in BuildState with explicit shutdown semantics",
      "kind": "Non-standard or antipatterns",
      "files": [
        "src/core/state.go"
      ],
      "short_description": "`src/core/state.go` has helpers like `addPendingParse`, `addPendingBuild`, and `addPendingTest` that start a goroutine solely to send into a channel and then use `defer recover()` to swallow panics from \"send on closed channel\". This is a non-idiomatic pattern that hides genuine lifecycle bugs and makes shutdown behaviour hard to reason about.\n\n",
      "recommended_action": "1. Decide on explicit channel lifecycle semantics:\n   - Clarify when `pendingParses`, `pendingActions`, and the test-related channels are closed and what should happen to new tasks after that point (e.g. return error, ignore tasks, or block until state is reinitialised).\n\n2. Replace goroutine+recover with safer sending:\n   - In `addPendingParse`, `addPendingBuild`, `addPendingTest` (in `src/core/state.go`):\n     - Remove the inline goroutine and `defer recover`.\n     - Use one of:\n       - Non-blocking `select { case ch <- task: default: ... }` with a clear behaviour when the channel is full or likely to be closing.\n       - A shared worker goroutine responsible for reading from a buffered internal queue and writing to the underlying channel, with explicit shutdown.\n     - Consider using a `context.Context` or an atomic `closing` flag on the `BuildState` so enqueue functions can check `if state.closing { return ErrStateClosed }` instead of risking a send on a closed channel.\n\n3. Add tests or integration tests for shutdown:\n   - Simulate closing these channels and calling the add* functions; assert that no panic occurs and the functions behave in a predictable, documented way (e.g. return an error or no-op).\n\nThis makes concurrency behaviour around the build/test queues more deterministic and removes a subtle source of racey panics that are currently being masked."
    },
    {
      "title": "Refactor buildTarget and build_step output handling to reduce complexity and duplication",
      "kind": "Spaghetti code",
      "files": [
        "src/build/build_step.go",
        "src/build/incrementality.go"
      ],
      "short_description": "The core build orchestration function `buildTarget` in `src/build/build_step.go` is large and contains many intertwined responsibilities (cache lookup, remote vs local execution, metadata and hash updates, output movement, post-build functions). There is also duplicated output-move/copy logic across several helpers. This raises the risk of regression when changing behaviour and makes it harder for new contributors to work on the build pipeline.\n\n",
      "recommended_action": "1. Split `buildTarget` into clearly defined phases:\n   - Identify major phases inside `buildTarget` (e.g. prepare, cache check, build/execute, post-build, store results) and extract them into helper functions like:\n     - `prepareBuild(state *core.BuildState, target *core.BuildTarget) error`\n     - `maybeRetrieveFromCache(...) (hit bool, err error)`\n     - `runLocalBuild(...) error` / `runRemoteBuild(...) error`\n     - `finalizeBuild(...) error` (metadata/hashes, cache store, etc.).\n   - Keep `buildTarget` as a high-level orchestrator that wires these pieces together and handles logging.\n\n2. Unify output movement/copying logic:\n   - Consolidate logic from functions like `moveOutputs`, `moveOutput`, `copyOutDir`, and `addOutputDirectoryToBuildOutput` (all in `src/build/build_step.go`) into a small, reusable set of helpers:\n     - e.g. `ensureParentDirs(path string) error`, `moveOrCopy(src, dst string) (changed bool, err error)`.\n   - Apply these helpers consistently across file and directory outputs.\n\n3. Centralize rule-hash and metadata handling:\n   - In `src/build/incrementality.go` and associated callers in `build_step.go`, wrap operations for reading/writing rule hashes and build metadata into a small abstraction (e.g. `type MetadataStore` with `ReadRuleHash`, `WriteRuleHash`, `LoadMetadata`, `StoreMetadata`).\n   - Ensure xattr-vs-file fallbacks and special handling for HTTP cache are implemented once in this layer.\n\n4. Standardize `(changed bool, err error)` patterns:\n   - Where functions currently return a bool + error to indicate \"changed\" vs \"no change\" (e.g. filegroup builder, `moveOutput`, cache retrieval helpers), either:\n     - Introduce a small result struct or\n     - At least document the semantics and apply them consistently across similar helpers.\n\n5. Add or extend tests:\n   - Unit tests for each new phase helper to verify behaviour in key scenarios: cache hit, cache miss, remote build failure, output move failure, metadata/hash write errors.\n\nThis refactor reduces \"spaghetti\" in the heart of the build loop and makes it much easier to modify specific aspects (like remote execution or output handling) without risking unrelated behaviour."
    },
    {
      "title": "Make repository locking APIs return errors instead of exiting and encapsulate lock state",
      "kind": "Non-standard or antipatterns",
      "files": [
        "src/core/lock.go"
      ],
      "short_description": "Repository locking in `src/core/lock.go` uses package-level state (`repoLockFile`) and functions like `AcquireSharedRepoLock` / `AcquireExclusiveRepoLock` that call `log.Fatal` on failure. This forces immediate process exit from within a library package and makes the locking behaviour less testable and harder to reuse.\n\n",
      "recommended_action": "1. Change the public locking API to return errors:\n   - Update `AcquireSharedRepoLock` and `AcquireExclusiveRepoLock` to return `error` instead of calling `log.Fatal`.\n   - Adjust all call sites (likely in `main` and high-level CLI code) to handle these errors and decide whether to log and exit or recover/continue.\n\n2. Encapsulate lock state in a type:\n   - Introduce `type RepoLock struct { file *os.File; mu sync.Mutex }` to manage the lock file instead of using a single package-level variable.\n   - Provide methods like `func NewRepoLock(path string) (*RepoLock, error)` and `func (l *RepoLock) AcquireShared() error`, `AcquireExclusive() error`, `Release() error`.\n\n3. Isolate platform-specific locking calls:\n   - Wrap `syscall.Flock` usage in an internal helper or interface to clarify platform support and, if needed, to allow mocking in tests.\n\n4. Tests:\n   - Add tests that:\n     - Attempt to acquire a lock twice and assert that the second call returns an error (not a process exit).\n     - Verify that `Release` can be called safely and no global state leakage occurs between tests.\n\nThis makes locking failures explicit, improves testability and avoids surprising process termination deep inside core packages."
    },
    {
      "title": "Unify cache key encoding and path/URL construction to avoid brittle assumptions",
      "kind": "Non-standard or antipatterns",
      "files": [
        "src/cache/dir_cache.go",
        "src/cache/http_cache.go",
        "src/cache/cmd_cache.go"
      ],
      "short_description": "Different cache backends build filesystem paths and URLs by concatenating package/target names and encoded keys, and some code (especially in `dir_cache`) relies on hard-coded expectations about encoded key length and padding. This is brittle if hash algorithms or formats change and makes path safety assumptions implicit.\n\n",
      "recommended_action": "1. Centralize key encoding:\n   - Create a helper (e.g. in `src/cache/keys.go`) that exposes something like:\n     - `func EncodeKey(key []byte) string`\n     - `func DecodeKey(s string) ([]byte, error)`\n   - Use this in all backends (dir, HTTP, cmd) instead of each encoding the key differently (base64 vs hex, etc.).\n\n2. Avoid heuristics based on encoded length for cleanup:\n   - In `src/cache/dir_cache.go`, replace logic that infers \"cache object\" vs other files based on length/padding of base64 strings.\n   - Track metadata explicitly if needed (e.g. store a small manifest containing algorithm/version) rather than relying on string shape.\n\n3. Sanitize filesystem path components:\n   - When deriving disk paths from package/target names, normalize and validate them to prevent unexpected separators or `..` components from influencing directory layout. If labels are already constrained by the BUILD language, make that assumption explicit and defended at the boundary where paths are built.\n\n4. Use `net/url` for HTTP cache URLs:\n   - In `src/cache/http_cache.go`, build URLs using `net/url.URL` and `ResolveReference` or path-joining rather than string concatenation. This ensures correct escaping and avoids subtle double-slash issues.\n\n5. Tests:\n   - Round-trip tests for key encoding/decoding.\n   - Tests for cleanup routines to confirm they operate only on valid cache entries.\n\nThis simplifies future changes to hash/key schemes and improves correctness and path safety in cache implementations."
    }
  ]
}